# -*- coding: utf-8 -*-
"""Fashion-Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_mhR3ihaHK6eX7SUhLci4CRGzYgEdh1b

Massimo Andreetta, Fabiana Rapicavoli
"""

import numpy as np
from urllib.request import urlopen
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_val_score
from sklearn import metrics
from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler 
import tensorflow 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Input, Dense 
from tensorflow.keras.utils import to_categorical 
from keras.utils.vis_utils import plot_model
from tensorflow.python.framework.random_seed import set_random_seed
from keras.callbacks import EarlyStopping
from keras import regularizers
from keras.layers.core import Dropout

# Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. 
# Each example is a 28x28 grayscale image, associated with a label from 10 classes.

def load_fashion_mnist():

    url_base = "https://www.math.unipd.it/~dasan/"
    Y_train = np.frombuffer(urlopen(url_base + "train-labels-idx1-ubyte").read(), dtype=np.uint8, offset=8)
    X_train = np.frombuffer(urlopen(url_base + "train-images-idx3-ubyte").read(), dtype=np.uint8, offset=16).reshape(len(Y_train), 784) 
                                                                                          
    Y_test = np.frombuffer(urlopen(url_base + "t10k-labels-idx1-ubyte").read(), dtype=np.uint8, offset=8)
    X_test = np.frombuffer(urlopen(url_base + "t10k-images-idx3-ubyte").read(), dtype=np.uint8, offset=16).reshape(len(Y_test), 784)

    return X_train, Y_train, X_test, Y_test

X_train_val, Y_train_val, X_test, Y_test = load_fashion_mnist()
print(X_train_val.shape, Y_train_val.shape, X_test.shape, Y_test.shape)

"""To speed up the learning operations we decided to reduce the dataset to 10% of its original size. 

For this purpose we used train_test_split on X_train and 
Y_train with test_size=0.9 and stratify = Y_train to keep the classes balanced.
"""

# Reduce the data to 10% to speed up the operations
X_reduced, _, Y_reduced, _ = train_test_split(X_train_val, Y_train_val, test_size=0.9, stratify = Y_train_val, random_state = 123)
#Stratify keeps the classes balanced
print(X_reduced.shape, Y_reduced.shape)

# Split data into 80% train+validation and 20% test
x_train_val, x_test, y_train_val, y_test = train_test_split(X_reduced, Y_reduced, test_size=0.2, shuffle=True, random_state = 123)
print(x_train_val.shape, y_train_val.shape, x_test.shape, y_test.shape)

# Split data into 80% train and 20% validation
x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, shuffle=True, random_state = 123)
print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)

Counter(Y_reduced) # To see if the classes are balanced after the reduction

#train the decision tree
dt_grid_params = {
    'criterion': ['entropy'],
    'max_depth': (1, 5, 10, 25),
}

dt_clf = DecisionTreeClassifier(random_state= 123)
dt_v1 = GridSearchCV(dt_clf, dt_grid_params, n_jobs= -1, cv = 5)
dt_v1.fit(x_train, y_train)
print(dt_v1.best_params_)

#train the random forest
rf_grid_params = {
    'n_estimators': (10, 25, 50),
    'criterion': ['entropy'],
    'max_depth': (1, 5, 25),
}

rf_clf = RandomForestClassifier(random_state= 123)
rf_v1 = GridSearchCV(rf_clf, rf_grid_params, n_jobs= -1, cv = 5)
rf_v1.fit(x_train, y_train)
print(rf_v1.best_params_)

#train the svc
svc_grid_params = {
    'C': (0.1, 1., 10),
    'kernel': ('rbf', 'linear'),
}

svc_clf = SVC(random_state= 123)
svc_v1 = GridSearchCV(svc_clf, svc_grid_params, n_jobs= -1, cv = 5)
svc_v1.fit(x_train, y_train)
print(svc_v1.best_params_)

def compute_f1score(true_values, pred_values):
  return f1_score(true_values, pred_values,average = 'macro')

# Decision Tree
y_train_pred = dt_v1.predict(x_train)
y_val_pred = dt_v1.predict(x_val)
DT_train = compute_f1score(y_train, y_train_pred)
DT_val = compute_f1score(y_val, y_val_pred)
print(f"Decision Tree.\tTrain:{DT_train:.4f}\tVal:{DT_val:.4f}")
# Train:0.9356	Val:0.7423

# Random Forest
y_train_pred = rf_v1.predict(x_train)
y_val_pred = rf_v1.predict(x_val)
RF_train = compute_f1score(y_train, y_train_pred)
RF_val = compute_f1score(y_val, y_val_pred)
print(f"Random Forest.\tTrain:{RF_train:.4f}\tVal:{RF_val:.4f}")
# Train:1.0000	Val:0.8335

# SVC
y_train_pred = svc_v1.predict(x_train)
y_val_pred = svc_v1.predict(x_val)
SVC_train = compute_f1score(y_train, y_train_pred)
SVC_val = compute_f1score(y_val, y_val_pred)
print(f"SVC.\tTrain:{SVC_train:.4f}\tVal:{SVC_val:.4f}")
# Train:0.9894	Val:0.8546

"""SVC appears to be the best performing model"""

y_train_pred = svc_v1.predict(x_train_val)
y_test_pred = svc_v1.predict(x_test)
print(f"SVC.\tTrain:{compute_f1score(y_train_val, y_train_pred):.4f}\tTest:{compute_f1score(y_test, y_test_pred):.4f}")
# Train:0.9628	Test:0.8450

y_test_pred = svc_v1.predict(x_test)
cm = confusion_matrix(y_test, y_test_pred, labels=svc_v1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc_v1.classes_)
disp.plot()
plt.show()

#Label 	Description
#  0 	T-shirt/top
#  1 	Trouser
#  2 	Pullover
#  3 	Dress
#  4 	Coat
#  5 	Sandal
#  6 	Shirt
#  7 	Sneaker
#  8 	Bag
#  9 	Ankle boot

"""Most of the errors comes from the misclassification of class 6 (shirt) that get understandably confused with class 0 and 2 (T-shirt and dress).
Let's now model the Neural Network
"""

#define a scaler
scaler = StandardScaler()
x_train_scl = scaler.fit_transform(x_train)
x_test_scl = scaler.fit_transform(x_test)
x_val_scl = scaler.fit_transform(x_val)
# simple early stopping
es = EarlyStopping(monitor='val_loss', 
                   mode='min',  
                   patience = 3, 
                   verbose=1)

#define the NN settings
feature_vector_length = len(x_train[0])
num_classes = 10

#convert the ground truth from numerical to cateogrical representation
y_train_cat = to_categorical(y_train, num_classes)
y_test_cat = to_categorical(y_test , num_classes)

"""Here we have all the most relevant previous versions of the Neural Network that we tried"""

np.random.seed(123)
set_random_seed(2)

model1 = Sequential() 
model1.add(Dense(input_dim = feature_vector_length, units=feature_vector_length, activation='relu')) #input layer
model1.add(Dense(units = 256, activation = 'relu'))
model1.add(Dense(num_classes, activation='softmax')) #output layer

# Configure the model and start training
model1.compile(loss='categorical_crossentropy', 
    optimizer='sgd',  
    metrics=['accuracy']) 

history1 = model1.fit(x_train, y_train_cat, epochs=500, batch_size=16, verbose=0, validation_split=0.1, callbacks=[es])

y_train_pred = model1.predict(x_train).argmax(axis = 1) 
y_val_pred = model1.predict(x_val).argmax(axis = 1)
print(f"NN.\tTrain:{f1_score(y_train, y_train_pred,average = 'macro'):.4f}\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}")
test_results = model1.evaluate(x_test, y_test_cat, verbose=0)
print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {test_results[1]:.4f}')
# No StandardScaler, 256 units
# Epoch 00003: early stopping
# Train:0.0181	Val:0.0185
# Test results - Loss: nan - Accuracy: 0.1000

np.random.seed(123)
set_random_seed(2)

model2 = Sequential() 
model2.add(Dense(input_dim = feature_vector_length, units=feature_vector_length, activation='relu')) #input layer
model2.add(Dense(units = 256, activation = 'relu'))
model2.add(Dense(num_classes, activation='softmax')) #output layer

# Configure the model and start training
model2.compile(loss='categorical_crossentropy', 
    optimizer='sgd',  
    metrics=['accuracy']) 

history2 = model2.fit(x_train_scl, y_train_cat, epochs=500, batch_size=16, verbose=0, validation_split=0.1, callbacks=[es])

y_train_pred = model2.predict(x_train_scl).argmax(axis = 1) 
y_val_pred = model2.predict(x_val_scl).argmax(axis = 1)
print(f"NN.\tTrain:{f1_score(y_train, y_train_pred,average = 'macro'):.4f}\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}")
test_results = model2.evaluate(x_test_scl, y_test_cat, verbose=0)
print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {test_results[1]:.4f}')
# StandardScaler, 256 units
# Epoch 00008: early stopping
# Train:0.9402	Val:0.8335
# Test results - Loss: 0.47644 - Accuracy: 0.8400

np.random.seed(123)
set_random_seed(2)

model3 = Sequential() 
model3.add(Dense(input_dim = feature_vector_length, units=feature_vector_length, activation='relu')) #input layer
model3.add(Dense(units = 64, activation = 'relu'))
model3.add(Dense(num_classes, activation='softmax')) #output layer

# Configure the model and start training
model3.compile(loss='categorical_crossentropy', 
    optimizer='sgd',  
    metrics=['accuracy']) 

history3 = model3.fit(x_train_scl, y_train_cat, epochs=500, batch_size=16, verbose=0, validation_split=0.1, callbacks=[es])

y_train_pred = model3.predict(x_train_scl).argmax(axis = 1) 
y_val_pred = model3.predict(x_val_scl).argmax(axis = 1)
print(f"NN.\tTrain:{f1_score(y_train, y_train_pred,average = 'macro'):.4f}\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}")
test_results = model3.evaluate(x_test_scl, y_test_cat, verbose=0)
print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {test_results[1]:.4f}')
# StandardScaler, 64 units
# Epoch 00008: early stopping
# Train:0.9334	Val:0.8329
# Test results - Loss: 0.4828 - Accuracy: 0.8425

"""Our final configuration with Dropout and l2 regularization to reduce a bit the overfitting"""

np.random.seed(123)
set_random_seed(2)

model4 = Sequential() 
model4.add(Dense(input_dim = feature_vector_length, units=feature_vector_length ,kernel_regularizer=regularizers.l2(0.01), activation='relu')) #input layer
model4.add(Dropout(0.1))
model4.add(Dense(units = 64, kernel_regularizer=regularizers.l2(0.01), activation = 'relu'))
model4.add(Dropout(0.1))
model4.add(Dense(num_classes, activation='softmax')) #output layer

# Configure the model and start training
model4.compile(loss='categorical_crossentropy', 
    optimizer='sgd',  
    metrics=['accuracy']) 

history4 = model4.fit(x_train_scl, y_train_cat, epochs=500, batch_size=32, verbose=0, validation_split=0.1, callbacks=[es])

y_train_pred = model4.predict(x_train_scl).argmax(axis = 1) 
y_val_pred = model4.predict(x_val_scl).argmax(axis = 1)
NN_train = compute_f1score(y_train, y_train_pred)
NN_val = compute_f1score(y_val, y_val_pred)
print(f"NN.\tTrain:{NN_train:.4f}\tVal:{NN_val:.4f}")
test_results = model4.evaluate(x_test_scl, y_test_cat, verbose=0)
print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {test_results[1]:.4f}')
# StandardScaler, 64 units, Dropout(0.1), regularizers.l2(0.01), batch_size=32
# Epoch 00120: early stopping
# Train:0.9762	Val:0.8507
# Test results - Loss: 0.7010 - Accuracy: 0.8475

"""Plots of training and validation accuracy and loss

"""

# summarize history for accuracy
plt.plot(history4.history['accuracy'])
plt.plot(history4.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
# plt.ylim(0.8, 1)
plt.show()


# summarize history for loss
plt.plot(history4.history['loss'])
plt.plot(history4.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Scatter plot of the accuracy scores of the four models"""

# Decision Tree
plt.scatter(DT_train, DT_val)
plt.annotate("Decision Tree", (DT_train, DT_val))
# Random Forest
plt.scatter(RF_train, RF_val)
plt.annotate("Random Forest", (RF_train, RF_val))
# SVC
plt.scatter(SVC_train, SVC_val)
plt.annotate("SVC", (SVC_train, SVC_val))
# Neural Network
plt.scatter(NN_train, NN_val)
plt.annotate("Neural Network", (NN_train, NN_val))

plt.title('F1 score of the different models')
plt.ylabel('Validation set F1 score')
plt.xlabel('Training set F1 score')

plt.show

"""It's seems like the best model for our task is the SVC.


Unfortunately, we've realised that SVC doesn't support dataset larger than 10000 samples, so we decided to use neural network, instead.
"""

# Split data into 80% train and 20% validation
X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, shuffle=True, random_state = 123)
print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)

num_classes=10
feature_vector_length = len(X_train[0])


es = EarlyStopping(monitor='val_loss', 
                   mode='min',  
                   patience = 3, 
                   verbose=1)

"""Let's model the Neaural Network with the entire dataset:"""

#NN with the entire dataset

#define a scaler
scaler2 = StandardScaler()
X_train_scl = scaler2.fit_transform(X_train)
X_test_scl = scaler2.fit_transform(X_test)
X_val_scl = scaler2.fit_transform(X_val)

#convert the ground truth from numerical to cateogrical representation
Y_train_cat = to_categorical(Y_train, num_classes)
Y_test_cat = to_categorical(Y_test , num_classes)

np.random.seed(123)
set_random_seed(2)

model = Sequential() 
model.add(Dense(input_dim = feature_vector_length, units=feature_vector_length ,kernel_regularizer=regularizers.l2(0.01), activation='relu')) #input layer
model.add(Dropout(0.1))
model.add(Dense(units = 64, kernel_regularizer=regularizers.l2(0.01), activation = 'relu'))
model.add(Dropout(0.1))
model.add(Dense(num_classes, activation='softmax')) #output layer

model.compile(loss='categorical_crossentropy', 
    optimizer='sgd',  
    metrics=['accuracy'])

history = model.fit(X_train_scl, Y_train_cat, epochs=500, batch_size=128, verbose=0, validation_split=0.1, callbacks=[es])

Y_train_pred = model.predict(X_train_scl).argmax(axis = 1) 
Y_val_pred = model.predict(X_val_scl).argmax(axis = 1)
print(f"NN.\tTrain:{compute_f1score(Y_train, Y_train_pred):.4f}\tVal:{compute_f1score(Y_val, Y_val_pred):.4f}")
test_results = model.evaluate(X_test_scl, Y_test_cat, verbose=0)
print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {test_results[1]:.4f}')

#other metrics: precision, recall
macro_precision, macro_recall, _, _ = precision_recall_fscore_support(Y_val, Y_val_pred, average='macro', zero_division = 0)
print(f'Test results - Precision: {macro_precision:.4f} - Recall: {macro_recall:.4f}')
# Epoch 00066: early stopping
# Train:0.9021	Val:0.8839
# Test results - Loss: 0.4855 - Accuracy: 0.8737
# Test results - Precision: 0.8845 - Recall: 0.8841

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
# plt.ylim(0.8, 1)
plt.show()


# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()